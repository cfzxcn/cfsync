**==Minio==、HDFS、==Ceph==、==GlusterFS==、FastDFS 开源分布式存储怎么选？**
# 三种存储方式
## 文件存储--Filestore
典型设备：FTP、NFS、NAS
文件存储：采用树状目录结构来管理和组织数据，即我们常见的文件和文件夹形式。这种存储方式提供了文件系统层次结构用户和应用程序可以通过文件路径进数据访问。文件存储系统支持多种文件系统协议，如NFS（网络文件系统）、CIFS（通用互联网文件系统）或SMB（服务器消息块）。它非常适合于多个用户共享文件、备份以及传统应用的部署，比如网络附加存储（NAS）设备或云文件存储服务。
文件系统是建立在操作系统之上的，是在操作系统层面上进行的封装。而操作系统负责操作数据块，
## 块存储
典型设备：磁盘阵列、硬盘、SSD
块存储是一种以固定大小的数据块为基本单位的存储方式，通常用于底层硬件与操作系统之间交互。块存储提供的是原始的、未经格式化的磁盘空间，类似于物理硬盘。操作系统可以将这些块视为连续的地址空间，并通过块设备驱动程序对其实现读写操作。块存储的主要特点是低延迟、高性能，特别适合那些需要频繁执行随机I/O操作的应用，如数据库系统和虚拟机的存储。在云计算环境中，块存储的一个典型例子是云硬盘服务，用户可以像在本地服务器上挂载硬盘一样挂载到虚拟机中
数据块通常和底层的操作系统进行交互。而操作系统之上建立的文件系统才是我们平常用到的文件
## 块存储与文件系统的关系
块存储本身并不依赖于文件系统来存储数据，它是以原始块级别提供给操作系统或者应用程序的，可以理解为是对底层存储介质（如硬盘）的直接访问。因此，在某些场景下，块存储是可以脱离文件系统独立使用的。例如，在虚拟化环境中，块存储可以被直接提供给虚拟机作为其虚拟磁盘，虚拟机操作系统安装在其上时，首先需要对这个虚拟磁盘进行分区和格式化，建立文件系统之后才能使用。在这个过程中，块存储在被格式化之前是不含有任何文件系统的。而在数据库系统中，数据库引擎可以直接管理块存储，将其划分为合适大小的数据块，并直接在这些块上存储数据库页，不需要通过文件系统的中间层。另外，在一些特定的应用场景中，如OracleRAC（RealApplicationClusters）或数据库裸设备使用时，为了提高性能和简化管理，数据库可以直接在裸的块存储上操作，无需经过文件系统的层级。然而，对于大多数普通用途而言，块存储通常不会单独使用，而是先格式化成某种文件系统，以便操作系统和应用程序能够理解和管理存储在其上的数据。
块存储和文件系统是下游基础设施和上游用户访问间的关系，它们抽象的层次不一样，块是设备本身提供的，是最基础的，文件系统是建立在操作系统基础之上，用于和用户方便交互的。现在主要通过文件来进行操作，这是最常用的场景。
## 对象存储-OS Object storage
典型设备：Minio
互联网海量的非结构化数据的存储需求，如：
- 电商网站：海量商品图片
- 视频网站：海量视频文件
- 音乐网站：海量音频文件
- 社交网站：海量图片
- 网盘：海量文件
- 云平台：大量的虚拟机镜像文件
以上海量数据如果是基于传统的san和nas的方式进行存储，无论是性能、容量、可用性等方面已经力所不及
当前常用的解决方案是基于对象存储服务（Object Storage Service，OSS)实现
对象存储是一种基于对象的存储架构，每个对象都是一个包含数据（通常是任意大小的字节流）和相关元数据（如文件名、创建日期、访问控制列表等）的自包含实体。对象存储不遵循文件系统的层级结构，而是通过唯一的全局唯一标识符（UID）来访问数据。由于其扁平化的设计和内置的分布式特性，对象存储非常适用于大规模非结构化数据存储，具有很高的扩展性和持久性，同时提供灵活的访问控制和内容寻址能力。阿里云 oss、Amazon S3、Microsoft Azure Blob Storage和Google Cloud Storage等云服务商提供的服务就是对象存储的例子，广泛应用于静态网站托管、图片/视频存储、大数据分析和备份归档等领域。
对象存储并不直接面向设备，也不面对某一台具体的服务器，更像一种云的产品
### 对象存储oss（Object Storage Service）特点
- 对象存储是一种分布式、基于http的Restful方式操作的存储服务
- 数据作为单独的对象进行存储
- 应用通过唯一地址来识别每个单独的数据对象，可以在互联网任何位置通过URL方式对每个对象进行存储和访问，但不支持挂载
- 对象存储是无层次结构的数据存储方法，不同于其他传统的数据存储方法，不直接使用目录树，而是存在于平面地址空间内的同一级别
- 适合存储非结构化数据；比如：视频、图片、日志、文本文件等，通常用于云计算环境中
- 在上传大文件时，OSS会采用分片上传
- 适合读多写少的场景
### 分为公有云的OSS服务和私有云的OSS服务
- 公有云：阿里云的OSS，腾讯云的COS，天翼云的OSS，亚马逊的叫S3(2006年3月诞生)，Microsoft Azure Blob存储等公有云提供OSS服务  
- 私有云:MinIO，Ceph RGW
#### 阿里云OSS重要特性
https://www.aliyun.com/product/oss
- 版本控制
版本控制是针对存储空间（Bucket）级别的数据保护功能。开启版本控制后，针对数据的覆盖和删除操作将会以历史版本的形式保存下来。您在错误覆盖或者删除文件（Object）后，能够将Bucket中存储的object恢复到任意时刻的历史版本。更多信息，请参见版本控制概述。
- Bucket Policy
Bucket拥有者可通过BucketPolicy授权不同用户以何种权限访问指定的OSS资源。例如您需要进行跨账号或对匿名用户授权访问或管理整个Bucket或Bucket内的部分资源，或者需要对同账号下的不同RAM用户授予访问或管理Bucket资源的不同权限，例如只读、读写或完全控制的权限等。关于配置BucketPolicy的具体操作，请参见通过BucketPolicy授权用户访问指定资源。
- 跨区域复制
跨区域复制（Cross-Region Replication）是跨不同OSS数据中心（地域）的Bucket自动、异步（近实时）复制Object，它会将Object的创建、更新和删除等操作从源存储空间复制到不同区域的目标存储空间。跨区域复制功能满足Bucket跨区域容灾或用户数据复制的需求。更多信息，请参见跨区域复制概述。
- 数据加密
服务器端加密：上传文件时，OSS对收到的文件进行加密，再将得到的加密文件持久化保存；下载文件时，OSS自动将加密文件解密后返回给用户，并在返回的HTTP请求Header中，声明该文件进行了服务器端加密。更多信息，请参见服务器端加密。
客户端加密：将文件上传到OSS之前在本地进行加密。更多信息，请参见客户端加密。
- 数据永久保存：OSS默认永久保存上传到Bucket的数据
#### 阿里云OSS应用场景
- 图片和音视频等应用的海量存储
OSS可用于图片、音视频、日志等海量文件的存储。各种终端设备、Web网站程序、移动应用可以直接向OSS写入或读取数据。OSS支持流式写入和文件写入两种方式。
- 网页或者移动应用的静态和动态资源分离
利用海量互联网带宽，OSS可以实现海量数据的互联网并发下载。OSS提供原生的传输加速功能，支持上传加速、下载加速，提升跨国、跨洋数据上传、下载的体验。同时，OSS也可以结合CDN产品，提供静态内容存储、分发到边缘节点的解决方案，利用CDN边缘节点缓存的数据，提升同一个文件被同一地区客户大量重复并发下载的体验。
- 云端数据处理
上传文件到OSS后，可以配合智能媒体管理服务和图片处理服务进行云端的数据处理
# 主流分布式存储系统
## HDFS (Hadoop Distributed File System)
HDFS是Apache Hadoop生态系统中的分布式文件系统，用于存储和处理大数据量的文件。它采用了一种主/从体系结构，其中一个主节点控制整个系统，并将数据分成多个块分布在多个从节点上。它支持大文件和大数据集，并能够提供高吞吐量和容错性。
官网地址：https://hadoop.apache.org/
## Ceph
Ceph是一个由Red Hat维护的分布式存储系统，可用于创建可扩展的对象存储、块存储和文件存储解决方案。它使用RADOS（可扩展的对象存储守护进程）将数据分布在多个存储节点上，并提供了强大的数据一致性和容错性。Ceph支持多个接口，如S3，Swift和CephFS，以满足不同的应用需求。
官网地址：https://ceph.io/
## Minio
Minio是一个开源的分布式对象存储系统，可用于创建私有或公共云存储服务。它具有高性能和可扩展性，并可通过S3和SwiftAPI与现有应用程序集成。Minio的设计重点是轻量级和易于部署和管理。
官网地址：https://min.io/    https://min.org.cn/    https://github.com/minio/minio
MinlO是GlusterFS创始人之一Anand BabuPeriasamy发布的新的开源项目。是一个用GoLang语言开发的基于GNUAGPLv3开源协议的对象存储服务(Object Storage Service,OSS)
对象存储服务是一种海量、安全、低成本、高可靠的云存储服务，适合存放任意类型的文件。支持容量和处理能力弹性扩展，多种存储类型供选择，全面优化存储成本。
MinlO兼容亚马逊S3云存储服务接口，非常适合于存储大容量非结构化的数据，例如：图片、视频、日志文件、备份数据和容器/虚拟机镜像等
MinlO支持的一个对象文件可以是任意大小，从几kb到最大5T不等
MinlO是一个非常轻量的服务，可以很简单的和其他应用的结合，也支持各种操作系统，比如：Linux,Windows,Mac等
对于中小型企业的对象存储，如果不选择公有云存储，那么Minio是个不错的选择
MinlO除了直接作为对象存储使用，还可以作为云上对象存储服务的网关层，无缝对接到AmazonS3、MicroSoftAzure。国内的阿里巴巴、腾讯、百度、中国联通、华为、中国移动等9000多家企业也都在使用MinIO产品。
### Minlo特点
- MinIO提供高性能、与S3兼容的对象存储系统，让你自己能够构建自己的云储存服务。
- MinIO使用和部署非常简单，可以让您在最快的时间内实现下载到生产环境的部署。
- MinlO读写性能优异，高性能，MinlO是世界上最快的对象存储，没有之一。在32个NVMe驱动器节点和100Gbe网络上发布的GET/PUT结果超过325GiB/秒和165GiB/秒
- MinlO支持的对象文件小到几kb到最大5T，并实现了数据的高可用
- MinlO原生支持Kubernetes，它可用于每个独立的公共云、每个Kubernetes发行版、私有云和边缘的对象存储套件。
- MinIO是软件定义的，不需要购买其他任何硬件，在GNUAGPLv3下是100%开源的
### MinIO相关术语
[词汇表 — MinIO中文文档 | MinIO Linux中文文档](https://www.minio.org.cn/docs/minio/linux/glossary.html#term-16)
- 池：一组 minio server 节点，它们合并其驱动器和资源以支持对象存储和检索请求
- Object对象
存储到Minio的基本对象，如==文件==、字节流等任意数据
- Bucket桶
用来存储Object的逻辑空间。每个Bucket之间的数据是相互隔离的。对于客户端而言，就相当于一个存放文件的==顶层文件夹==，用于**实现不同资源的分类存储**，通常一个项目或同一类资源可以对应于一个Bucket
- Drive驱动器
即存储数据的**磁盘**，一个Drive通常对应一块物理磁盘或者一个独立目录（试验时可用目录来模拟磁盘）在。MinlO启动时，以参数的方式传入。Minio中所有的对象数据都会存储在Drive里
- Set存储集
一组Drive的集合，即一组相关的磁盘的集合。分布式部署时，MinIO会根据集群规模自动划分一个或多个Set，每个Set中的Drive分布在不同位置。一个对象存储在一个Set上。一个集群划分为多个Set。一个Set包含的Drive数量是固定的，默认由系统根据集群规模自动计算得出。一个SET中的Drive尽可能分布在不同的节点上
### 纠删码EC(Erasure Code)-类似raid
https://blog.min.io/erasure-coding/
https://min.io/docs/minio/linux/operations/concepts/erasure-coding.html
https://www.minio.org.cn/docs/minio/kubernetes/upstream/operations/concepts.html#id11
- MinlO使用纠删码EC(Erasure-Code)实现数据余和可靠性。
- MinlO纠删码EC(ErasureCode)是一种数据冗余和可用性功能，允许具有多个驱动器的MinlO部署即时自动重建对象，即使群集中丢失了多个驱动器或节点。纠删码提供了对象级修复
- MinlO使用纠删码机制来保证高可靠性，使用highwayhash来处理数据损坏(Bit Rot Protection)。
- Minio使用纠删码erasure code和校验和checksum来保护数据免受硬件故障和无声数据损坏。即便您丢失一半数量(N/2)的硬盘,您仍然可以恢复数据。
- 当损坏总磁盘数的**一半**磁盘（损坏磁盘不超过半数，数据仍能恢复）时，只能读取而不能上传新的文件，只要保证正常磁盘数大于等n/2+1时，就可以支持写入新数据，即正常读写
- 实现纠删码EC至少需要**4块磁盘**以上。**但minio不是必须使用ec的**。测试环境中也可用一块硬盘部署minio，但这样就不支持ec了，就没有高可用和冗余功能了。
- 纠删码是可以通过数学计算，实现数据冗余，功能上类似于RAID技术，当磁盘损坏时，可以通过计算把丢失的数据进行还原，它可以将n份原始数据，增加m份数据，并能通过n+m份中的任意n份数据，还原为原始数据。即如果有任意小于等于m份的数据失效，仍然能通过剩下的数据还原出来。
- MinlO将对象划分为块（称为分片），以块为单位对文件进行分块后进行存储，并将它们均匀分布在纠删码集中的每个驱动器之间。MinlO可以继续无缝地为读取和写入请求提供服务
- MinlO根据纠删集的大小将对象拆分为数据和奇偶校验块，然后将数据和奇偶校验块随机均匀地分布在一组驱动器上，以便每个驱动器每个对象包含的块不超过一个。虽然驱动器可能包含多个对象的数据和奇偶校验块，但只要系统中有足够的驱动器，单个对象每个驱动器的块不超过一个。对于版本化对象，MinlO选择相同的驱动器进行数据和奇偶校验存储，同时在任何一个驱动器上保持零重叠。
- 当一个大文件大于10 MB时，写入MinIO，S3（规范）API将其分解为分段上传。分段大小由客户端在上传时确定。S3要求每个部分至少为5MB（最后一部分除外）且不超过5GB。根据S10规范，一个对象最多可以有xxx个部件。假设一个320MB的对象。如果此对象分为64个部分，MinlO会将这些部分作为第1部分、第2部分写入驱动器..直到第64部分。这些部分的大小大致相等，例如，作为多部分上传的320MB对象将拆分为64个5MB部分。上传的每个部分都通过条带进行擦除编码。Part.1是上载对象的第一部分，所有部分在驱动器上水平条带化。每个部分都由其数据块、奇偶校验块和XL元数据组成。MinlO轮换写入，因此系统不会总是将数据写入相同的驱动器，并将奇偶校验写入相同的驱动器。每个对象都独立旋转，允许统一有效地使用群集中的所有驱动器，同时还增强了数据保护。
- 为了检索对象，MinlO执行哈希计算以确定对象的保存位置，读取哈希并访问所需的纠删节和驱动器。读取对象时，存在数据和奇偶校验块，如XL 元数据中所述
- 需要注意的是：纠删码的内在工作原理和RAID实际是不同的，像RAID5可以在损失一块盘的情况下不丢数据，而Minio纠删码可以在丢失一半的盘的情况下，仍可以保证数据安全。而且Minio纠删码是作用在对象级别，可以一次恢复一个对象，而RAID是作用在卷级别，数据恢复时间很长。Minio对每个对象单独编码，存储服务一经部署，通常情况下是不需要更换硬盘或者修复。MinlO纠删码的设计目标是为了性能和尽可能的使用硬件加速，与相类技术（如RAID或复制）相比，开销要少得多。
#### 纠删码计算器
https://min.io/product/erasure-code-calculator
- Number of Racks    几个机架，如果为1
- Number of Servers per Rack    每个机架上有几台服务器，如果为4
- Number of Drives per Server    每台服务器有几块硬盘，如果为4
- Drive Capacity   每个硬盘有多大容量，如果为10T
- Erasure Code Stripe Size (K+M)    纠删码条带集大小，如果为8
- Erasure Code Parity (M)    纠删码校验位，如果为2，那么数据位就是6，即存放数据的空间大，是75%
那么有效空间就是：`4*4*10=160TiB*75%=120 TiB`

Command Line Parameters：minio server https://node{1...4}.example.net/mnt/drive{1...4}
### MinIO工作流程
![[Pasted image 20240419013100.png|850]]
### MinI0启动模式
- 单机模式：standalone mode
	- non-erasure code mode：一台主机一块硬盘
	- erasure code mode：一台主机至少四块硬盘
- 分布式集群模式：distributed mode
	- erasure code mode：四台主机每台一块硬盘（可以，可用多个目录来模拟多个硬盘），还是，四台主机每台至少四块硬盘（可以）
### MinI0部署模式
#### MinIO 支持三种拓扑结构
- 单节点单硬盘
- 单节点多硬盘（至少四块硬盘）
- 多节点多硬盘（分布式集群部署）
#### 部署方法
- 包安装
- 二进制安装
- Docker容器化安装
- 基于Kubernetes部署
## FastDFS
FastDFS是一个开源的高性能分布式文件系统，用于存储大量文件和流式数据。它采用一种客户端/服务器体系结构，其中客户端通过TCP/IP协议与服务器交互。FastDFS支持水平扩展和负载均衡，并具有快速的文件上传和下载速度。
官网地址：https://github.com/happyfish100/fastdfs
**有点老了，该淘汰了！**
## GlusterlFS
# 分布式文件系统对比
![[Pasted image 20240416204405.png|850]]

![[Pasted image 20240416205033.png|800]] ![[Pasted image 20240416205302.png#R]]


















# FastDFS分布式文件系统
https://github.com/happyfish100/fastdfs
==适合中小型企业搭建图片服务器==
## 2.1 简介
FastDFS是一个开源的高性能分布式文件系统（DFS）。它的主要功能包括：文件存储，文件同步和文件访问，以及高容量和负载平衡。主要解决了海量数据存储问题，特别适合以中小文件（建议范围：4KB<file_size<500MB）为载体的在线服务。
FastDFS设计是==用来存储小文件的==，过大的文件处理方案是拆分为小文件，可跟踪小文件的上传情况。如果应用场景都是处理大文件，可能选择其他分布式文件系统方案会更合适。
## 2.2 特性
FastDFS为互联网量身定制，充分考虑了余备份、负载均衡、线性扩容等机制，并注重高可用、高性能等指标，使用FastDFS很容易搭建一套高性能的文件服务器集群提供文件上传、下载等服务。
### 优点：
- 文件不分块存储，文件和系统中的文件一一对应。
- 对文件内容做hash处理，避免出现重复文件，节约磁盘空间。
- 下载文件支持HTTP协议，可基于内置WebServer或外部WebServer。
- 支持在线扩容，动态添加卷。
- 支持文件冗余备份和负载均衡。
- 存储服务器上可以保存文件属性（meta-data）
- V2.0网络通信采用libevent，支持大并发访问，整体性能更好。
### 缺点：
- 直接按文件存储，可直接查看文件内容，缺乏文件安全性。
- 数据同步无校验，存在静默IO问题，降低系统可用性。
- 单线程数据同步，仅适合存储小文件（4KB到500MB之间）
- 备份数根据存储分卷（分组）决定，缺乏文件备份数设置灵活性
- 单个挂载点异常会导致整个存储节点下线。
- 缺乏多机房容灾支持。
- 静态的负载均衡机制。
## 2.3 角色
- TrackerServer：跟踪服务器，主要做调度工作，起到均衡的作用；负责管理所有的storageserver和group，每个storage在启动后会连接Tracker，告知自己所属group等信息，并保持周期性心跳。
- Storage Server：存储服务器，主要提供容量和备份服务；以 group 为单位，每个group 内可以有多台 storage server，数据互为备份。
- Client：客户端，上传下载数据的服务器，也就是我们自己的项目所部署在的服务器。
Tracker相当于一个调度中心，上传和下载都通过它来进行分配指定。
Storagecluster部分，由Volume1、Volume2....VolumeK组成，它们称为卷（或者叫做组），卷与卷之间是平行的关系，可以根据资源的使用情况随时增加，卷内服务器文件相互同步备份，以达到容灾的目的。
## 2.4 存储策略
为了支持大容量，存储节点（服务器）采用了分卷（或分组）的组织方式。r存储系统由一个或多个卷组成，卷与卷之间的文件是相互独立的，所有卷的文件容量累加就是整个存储系统中的文件容量。一个卷可以由一台或多台存储服务器组成，一个卷下的存储服务器中的文件都是相同的，卷中的多台存储服务器起到了余备份和负载均衡的作用。

在卷中增加服务器时，同步已有的文件由系统自动完成，同步完成后，系统自动将新增服务器切换到线上提供服务。当存储空间不足或即将耗尽时，可以动态添加卷。只需要增加一台或多台服务器，并将它们配置为一个新的卷，这样就扩大了存储系统的容量。
## 2.5上传过程
当服务启动之后，StorageServer会定期的向TrackerServer发送存储信息。如果TrackerServer是集群形式，则每个Tracker之间的关系是对等的，客户端上传时选择任意一个Tracker即可。

整体流程：当客户端请求Tracker进行上传操作时，会获取存储服务器相关信息，主要包括IP和端口。根据返回信息上传文件，通过存储服务器写入磁盘，并返回给客户端file_id、路径信息、文件名等信息。

其中，当Tracker收到客户端上传文件的请求时，会为该文件分配一个可以存储文件的group，当选定了group后就要决定给客户端分配group中的哪一个storageserver。当分配好storageserver后，客户端向storage发送写文件请求，storage将会为文件分配一个数据存储目录。然后为文件分配一个fileid，最后根据以上的信息生成文件名存储文件。生成的文件名基本格式如下：
![[Pasted image 20240415123026.png]]
- 组名：文件上传后所在的storage组名称，在文件上传成功后有storage服务器返回，需要客户端自行保存。
- 虚拟磁盘路径：storage配置的虚拟路径，与配置文件中的磁盘选项store_path\*对应。如果配置了store_path0 则是M00，如果配置了store_path1 则是M01，以此类推。
- 数据两级目录：storage服务器在每个虚拟磁盘路径下创建的两级目录，用于存储数据文件。
- 文件名：与文件上传时不同。是由存储服务器根据特定算法生成，文件名包含：源存储服务器IP地址、文件创建时间戳、文件大小、随机数和文件拓展名等信息。
## 2.6 文件同步
写文件时，客户端将文件写至group内一个storage server即认为写文件成功，storage server写完文件后，会由后台线程将文件同步至同group内其他的storage server。
每个storage写文件后，同时会写份binlog，binlog里不包含文件数据，只包含文件名等元信息，这份binlog用于后台同步，storage会记录向group内其他storage同步的进度，以便重启后能接上次的进度继续同步；进度以时间戳的方式进行记录，所以最好能保证集群内所有server的时钟保持同步。
storage的同步进度会作为元数据的一部分汇报到tracker上，tracke在选择读storage的时候会以同步进度作为参考。
## 2.7 下载过程
跟上传一样，在下载时客户端可以选择任意Trackerserver。
客户端带文件名信息请求Tracker，Tracker从文件名中解析出文件的group、大小、创建时间等信息，然后选择一个storage用来服务处理请求，返回对应文件。




 











